{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 8304744,
          "sourceType": "datasetVersion",
          "datasetId": 4933374
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gowtham-P23/new_google/blob/main/Copy_of_ShearletExitUnet_semantic_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "aletbm_global_land_cover_mapping_openearthmap_path = kagglehub.dataset_download('aletbm/global-land-cover-mapping-openearthmap')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HS6B0P6GRUY",
        "outputId": "695140f5-fc06-4838-d39a-ca080b2df080"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/aletbm/global-land-cover-mapping-openearthmap?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8.47G/8.47G [01:19<00:00, 114MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q segmentation-models-pytorch\n",
        "!pip install -q albumentations\n",
        "!pip install -q timm\n",
        "!pip install -q rasterio"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-16T08:16:07.281939Z",
          "iopub.execute_input": "2025-10-16T08:16:07.282183Z",
          "iopub.status.idle": "2025-10-16T08:17:49.980158Z",
          "shell.execute_reply.started": "2025-10-16T08:16:07.28216Z",
          "shell.execute_reply": "2025-10-16T08:17:49.979298Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfRWecSRGRUa",
        "outputId": "45c222cf-dddf-4f01-9213-02855082d630"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import lr_scheduler # Import lr_scheduler\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import segmentation_models_pytorch as smp"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-16T08:17:55.41748Z",
          "iopub.execute_input": "2025-10-16T08:17:55.417982Z",
          "iopub.status.idle": "2025-10-16T08:17:56.525181Z",
          "shell.execute_reply.started": "2025-10-16T08:17:55.417956Z",
          "shell.execute_reply": "2025-10-16T08:17:56.523984Z"
        },
        "id": "gVlvn7tlGRUb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "class Config:\n",
        "    # DATA_PATH = \"/kaggle/input/global-land-cover-mapping-openearthmap/\"\n",
        "    DATA_PATH = aletbm_global_land_cover_mapping_openearthmap_path\n",
        "    # DATA_PATH = \"/root/.cache/kagglehub/datasets/aletbm/global-land-cover-mapping-openearthmap/versions/1/\"\n",
        "\n",
        "    # --- File/Directory Names ---\n",
        "    IMAGE_DIR_NAME = \"images\"\n",
        "    LABEL_DIR_NAME = \"label\"\n",
        "    TRAIN_TXT = os.path.join(DATA_PATH, \"train.txt\")\n",
        "    VAL_TXT = os.path.join(DATA_PATH, \"val.txt\")\n",
        "\n",
        "    # --- Base Directories ---\n",
        "    IMAGE_BASE_DIR = os.path.join(DATA_PATH, IMAGE_DIR_NAME)\n",
        "    LABEL_BASE_DIR = os.path.join(DATA_PATH, LABEL_DIR_NAME)\n",
        "\n",
        "    NUM_CLASSES = 9\n",
        "\n",
        "    # --- Training Hyperparameters ---\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    BATCH_SIZE = 4\n",
        "    NUM_WORKERS = 2\n",
        "    IMAGE_SIZE = 256\n",
        "    LEARNING_RATE = 1e-4\n",
        "    EPOCHS = 40\n",
        "    PIN_MEMORY = True\n",
        "    FPN_CHANNELS = 256\n",
        "    # Add Weight Decay for regularization\n",
        "    WEIGHT_DECAY = 1e-2\n",
        "    # Parameters for Early Stopping\n",
        "    EARLY_STOPPING_PATIENCE = 10 # Number of epochs to wait for improvement\n",
        "    MIN_DELTA = 0.001 # Minimum change to qualify as an improvement\n",
        "\n",
        "cfg = Config()\n",
        "print(f\"Using device: {cfg.DEVICE}\")\n",
        "print(f\"Number of classes: {cfg.NUM_CLASSES}\")\n",
        "print(f\"Weight Decay: {cfg.WEIGHT_DECAY}\")\n",
        "print(f\"Early Stopping Patience: {cfg.EARLY_STOPPING_PATIENCE}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-16T08:12:07.69281Z",
          "iopub.status.idle": "2025-10-16T08:12:07.693045Z",
          "shell.execute_reply.started": "2025-10-16T08:12:07.692937Z",
          "shell.execute_reply": "2025-10-16T08:12:07.692948Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SZujmzkGRUc",
        "outputId": "2a3020c5-5bc8-4d1b-bcd5-f2d6812cdda3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Number of classes: 9\n",
            "Weight Decay: 0.01\n",
            "Early Stopping Patience: 10\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations for training and validation sets using Albumentations\n",
        "train_transform = A.Compose([\n",
        "    # Use RandomResizedCrop instead of just Resize for better generalization\n",
        "    A.RandomResizedCrop(\n",
        "        size=(cfg.IMAGE_SIZE, cfg.IMAGE_SIZE),\n",
        "        scale=(0.7, 1.0),\n",
        "        p=1.0\n",
        "    ),\n",
        "\n",
        "    # Geometric augmentations\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomRotate90(p=0.5),\n",
        "    A.ShiftScaleRotate(\n",
        "        shift_limit=0.1,\n",
        "        scale_limit=0.15,\n",
        "        rotate_limit=15,\n",
        "        border_mode=0,\n",
        "        p=0.5\n",
        "    ),\n",
        "\n",
        "    # Spatial augmentations\n",
        "    A.OneOf([\n",
        "        A.ElasticTransform(alpha=1, sigma=50, p=1),\n",
        "        A.GridDistortion(num_steps=5, distort_limit=0.3, p=1),\n",
        "        A.OpticalDistortion(distort_limit=0.5, shift_limit=0.5, p=1),\n",
        "    ], p=0.3),\n",
        "\n",
        "    # Color augmentations\n",
        "    A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1, p=0.6),\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.4),\n",
        "\n",
        "    # Noise and blur\n",
        "    A.OneOf([\n",
        "        A.GaussNoise(var_limit=(10.0, 50.0), p=1),\n",
        "        A.GaussianBlur(blur_limit=(3, 7), p=1),\n",
        "    ], p=0.3),\n",
        "\n",
        "    # Simulate missing data/clouds\n",
        "    A.CoarseDropout(\n",
        "        max_holes=5,\n",
        "        max_height=32,\n",
        "        max_width=32,\n",
        "        min_holes=1,\n",
        "        min_height=8,\n",
        "        min_width=8,\n",
        "        fill_value=0,\n",
        "        p=0.25\n",
        "    ),\n",
        "\n",
        "    A.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225],\n",
        "        max_pixel_value=255.0,\n",
        "    ),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "val_transform = A.Compose([\n",
        "    A.Resize(height=cfg.IMAGE_SIZE, width=cfg.IMAGE_SIZE),\n",
        "    A.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406], # ImageNet mean\n",
        "        std=[0.229, 0.224, 0.225],  # ImageNet std\n",
        "        max_pixel_value=255.0,\n",
        "    ),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "class LandCoverDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for your Land Cover data - Enhanced version.\"\"\"\n",
        "\n",
        "    def __init__(self, image_paths, mask_paths, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.mask_paths = mask_paths\n",
        "        self.transform = transform\n",
        "        self.num_classes = cfg.NUM_CLASSES\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        mask_path = self.mask_paths[idx]\n",
        "\n",
        "        try:\n",
        "            # Read image\n",
        "            with rasterio.open(img_path, 'r') as f:\n",
        "                image = f.read([1, 2, 3]).transpose(1, 2, 0)\n",
        "\n",
        "                # Handle different dtypes\n",
        "                if image.dtype == np.uint16:\n",
        "                    image = (image / 256).astype(np.uint8)\n",
        "                elif image.dtype in [np.float32, np.float64]:\n",
        "                    image = (np.clip(image, 0, 1) * 255).astype(np.uint8)\n",
        "                else:\n",
        "                    image = image.astype(np.uint8)\n",
        "\n",
        "            # Read mask - FIXED: proper dtype handling\n",
        "            with rasterio.open(mask_path, 'r') as f:\n",
        "                mask = f.read(1).astype(np.int64)  # Changed from uint8 to int64\n",
        "\n",
        "            # Apply augmentations\n",
        "            if self.transform:\n",
        "                augmented = self.transform(image=image, mask=mask)\n",
        "                image = augmented[\"image\"]\n",
        "                mask = augmented[\"mask\"]\n",
        "\n",
        "            # Ensure mask is long tensor - FIXED: proper conversion\n",
        "            if isinstance(mask, np.ndarray):\n",
        "                mask = torch.from_numpy(mask).long()\n",
        "            else:\n",
        "                mask = mask.long()\n",
        "\n",
        "            return image, mask\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {img_path}: {e}\")\n",
        "            # Return dummy data to avoid crash\n",
        "            return (\n",
        "                torch.zeros(3, cfg.IMAGE_SIZE, cfg.IMAGE_SIZE),\n",
        "                torch.zeros(cfg.IMAGE_SIZE, cfg.IMAGE_SIZE, dtype=torch.long)\n",
        "            )"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-16T08:12:07.694019Z",
          "iopub.status.idle": "2025-10-16T08:12:07.694551Z",
          "shell.execute_reply.started": "2025-10-16T08:12:07.694332Z",
          "shell.execute_reply": "2025-10-16T08:12:07.69435Z"
        },
        "id": "WD-2kwivGRUe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2619f732-576a-430e-cef1-bd384fcb844b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
            "  original_init(self, **validated_kwargs)\n",
            "/tmp/ipython-input-3296406471.py:26: UserWarning: Argument(s) 'shift_limit' are not valid for transform OpticalDistortion\n",
            "  A.OpticalDistortion(distort_limit=0.5, shift_limit=0.5, p=1),\n",
            "/tmp/ipython-input-3296406471.py:35: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
            "  A.GaussNoise(var_limit=(10.0, 50.0), p=1),\n",
            "/tmp/ipython-input-3296406471.py:40: UserWarning: Argument(s) 'max_holes, max_height, max_width, min_holes, min_height, min_width, fill_value' are not valid for transform CoarseDropout\n",
            "  A.CoarseDropout(\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import timm\n",
        "import torch.nn.functional as F # Import functional\n",
        "\n",
        "class TimmEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder module using a pre-trained backbone from the timm library.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name='swin_tiny_patch4_window7_224', pretrained=True, in_chans=3):\n",
        "        super().__init__()\n",
        "        # create_model will return a model that outputs a list of feature maps\n",
        "        # at different resolutions, which is exactly what we need for skip connections.\n",
        "        self.backbone = timm.create_model(\n",
        "            model_name,\n",
        "            pretrained=pretrained,\n",
        "            features_only=True,\n",
        "            in_chans=in_chans,\n",
        "        )\n",
        "\n",
        "        self.output_channels = self.backbone.feature_info.channels()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # The backbone returns a list of feature maps, e.g., [64, 128, 256, 512] channels\n",
        "        return self.backbone(x)\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard U-Net decoder block.\n",
        "    Upsamples, concatenates with a skip connection, and applies convolutions.\n",
        "    Includes optional Dropout.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, skip_channels, out_channels, dropout_prob=0.2):\n",
        "        super().__init__()\n",
        "        # Upsampling layer (could also use nn.Upsample + nn.Conv2d)\n",
        "        self.upsample = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "\n",
        "        # Convolutional layers to process the concatenated features\n",
        "        self.convs = nn.Sequential(\n",
        "            nn.Conv2d(in_channels // 2 + skip_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout_prob) if dropout_prob > 0 else nn.Identity()\n",
        "\n",
        "\n",
        "    def forward(self, x, skip_connection):\n",
        "        x = self.upsample(x)\n",
        "        # Concatenate the upsampled features with the skip connection\n",
        "        x = torch.cat([x, skip_connection], dim=1)\n",
        "        x = self.convs(x)\n",
        "        x = self.dropout(x) # Apply dropout\n",
        "        return x\n",
        "\n",
        "class ShearletExitUNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, num_classes=9, shearlet_features=16, dropout_prob=0.2, fpn_out_channels=256, backbone_name='swin_tiny_patch4_window7_224'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lsf = LearnedShearletFrontEnd(in_channels, shearlet_features)\n",
        "        self.encoder = TimmEncoder(model_name=backbone_name, pretrained=True, in_chans=in_channels)\n",
        "\n",
        "        encoder_features_info = self.encoder.backbone.feature_info # e.g., [64, 128, 256, 512] channels\n",
        "        encoder_channels = [info['num_chs'] for info in encoder_features_info]\n",
        "\n",
        "\n",
        "        # --- 1. FPN Layer Initialization ---\n",
        "        # Lateral layers (1x1 convs) to process encoder features\n",
        "        self.lateral_convs = nn.ModuleList()\n",
        "        for in_chans in encoder_channels:\n",
        "            self.lateral_convs.append(\n",
        "                nn.Conv2d(in_chans, fpn_out_channels, kernel_size=1)\n",
        "            )\n",
        "\n",
        "        # Output layers (3x3 convs) to create the final feature pyramid\n",
        "        self.output_convs = nn.ModuleList()\n",
        "        for _ in range(len(encoder_channels)):\n",
        "            self.output_convs.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(fpn_out_channels, fpn_out_channels, kernel_size=3, padding=1),\n",
        "                    nn.ReLU(inplace=True)\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # --- 2. Custom Gates and Prediction Heads ---\n",
        "        self.cfs_gates = nn.ModuleList()\n",
        "        self.deep_supervision_heads = nn.ModuleList()\n",
        "\n",
        "        # We need gates and heads for each level of the pyramid\n",
        "        skip_chans = encoder_channels[::-1] # All encoder levels are used as skips in FPN\n",
        "\n",
        "        # The decoder input for the gate will be the FPN feature from the level above\n",
        "        gate_decoder_channels = [fpn_out_channels] * len(encoder_channels)\n",
        "\n",
        "        for i in range(len(encoder_channels)):\n",
        "            self.cfs_gates.append(\n",
        "                CrossFrequencySkipGate(skip_chans[i], gate_decoder_channels[i], shearlet_features)\n",
        "            )\n",
        "            self.deep_supervision_heads.append(\n",
        "                SegmentationHead(fpn_out_channels, num_classes)\n",
        "            )\n",
        "\n",
        "        # Note: The early-exit modules are omitted here for simplicity but could be\n",
        "        # added back, attached to the final FPN feature maps.\n",
        "\n",
        "    def forward(self, x, training=True, exit_threshold=0.8):\n",
        "        f_shear = self.lsf(x)\n",
        "        encoder_features = self.encoder(x) # List of features [c2, c3, c4, c5]\n",
        "        # Depending on the timm backbone, features might need permuting\n",
        "        # Check the output shape of your chosen backbone.\n",
        "        # For Swin Transformer, we permute:\n",
        "        # encoder_features = [f.permute(0, 3, 1, 2) for f in encoder_features]\n",
        "\n",
        "\n",
        "        # --- 3. FPN Top-Down Pathway Implementation ---\n",
        "\n",
        "        # Start with the deepest encoder feature (c5)\n",
        "        p5 = self.lateral_convs[-1](encoder_features[-1])\n",
        "        p5_out = self.output_convs[-1](p5)\n",
        "        pyramid_features = [p5_out]\n",
        "\n",
        "        # Loop from the second-to-last layer upwards\n",
        "        # zip(reversed(list_[:-1]), reversed(list_[:-1])) -> (c4, lateral4), (c3, lateral3)...\n",
        "        for i in range(len(encoder_features) - 2, -1, -1):\n",
        "            # Get previous pyramid feature (e.g., p5) and current encoder feature (e.g., c4)\n",
        "            p_prev = pyramid_features[0] # The latest feature added to the pyramid\n",
        "            c_curr = encoder_features[i]\n",
        "\n",
        "            # --- 4. Integrate Your Custom Gate ---\n",
        "            # The gate uses the feature from the layer above (p_prev) as its decoder context\n",
        "            gated_skip = self.cfs_gates[len(encoder_features) - 1 - i](c_curr, p_prev, f_shear)\n",
        "\n",
        "            # Upsample previous P and add to the (gated) current C\n",
        "            p_prev_upsampled = F.interpolate(p_prev, size=c_curr.shape[-2:], mode='bilinear')\n",
        "            p_curr = self.lateral_convs[i](gated_skip) + p_prev_upsampled\n",
        "\n",
        "            # Apply output conv and add to the front of the pyramid list\n",
        "            p_curr_out = self.output_convs[i](p_curr)\n",
        "            pyramid_features.insert(0, p_curr_out)\n",
        "\n",
        "        # --- 5. Generate Predictions from the Pyramid ---\n",
        "        all_predictions = []\n",
        "        # pyramid_features is now [p2, p3, p4, p5]\n",
        "        for i, p_out in enumerate(pyramid_features):\n",
        "             # The heads are in order [h2, h3, h4, h5]\n",
        "             # To match p2 -> h2, p3 -> h3 etc., we use deep_supervision_heads[i]\n",
        "            prediction = self.deep_supervision_heads[i](p_out)\n",
        "            all_predictions.append(prediction)\n",
        "\n",
        "        if training:\n",
        "            # Reverse the list so the deepest prediction is last, matching your previous loss logic\n",
        "            return all_predictions[::-1]\n",
        "        else:\n",
        "            # For inference, typically the highest-resolution prediction (from P2) is used\n",
        "            # after upsampling to the original image size.\n",
        "            final_pred = F.interpolate(all_predictions[0], size=x.shape[-2:], mode='bilinear')\n",
        "            return final_pred"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-16T08:12:07.696011Z",
          "iopub.status.idle": "2025-10-16T08:12:07.696349Z",
          "shell.execute_reply.started": "2025-10-16T08:12:07.696182Z",
          "shell.execute_reply": "2025-10-16T08:12:07.696196Z"
        },
        "id": "wi82CD3EGRUe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class LearnedShearletFrontEnd(nn.Module):\n",
        "    def __init__(self, in_channels, num_subbands=16):\n",
        "        super().__init__()\n",
        "        # Create a list of 1x1 convs to act as linear combinations,\n",
        "        # followed by 3x3 convs to learn spatial patterns for each sub-band.\n",
        "        self.subband_convs = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(in_channels, 1, kernel_size=1, padding='same'),\n",
        "                nn.Conv2d(1, 1, kernel_size=3, padding='same')\n",
        "            ) for _ in range(num_subbands)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply each conv to the input and stack the results\n",
        "        subband_features = [conv(x) for conv in self.subband_convs]\n",
        "        return torch.cat(subband_features, dim=1) # Shape: (B, C_f, H, W)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-16T08:12:07.698136Z",
          "iopub.status.idle": "2025-10-16T08:12:07.698453Z",
          "shell.execute_reply.started": "2025-10-16T08:12:07.698316Z",
          "shell.execute_reply": "2025-10-16T08:12:07.698331Z"
        },
        "id": "XOj2RpbMGRUf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossFrequencySkipGate(nn.Module):\n",
        "    def __init__(self, skip_channels, decoder_channels, shearlet_channels):\n",
        "        super().__init__()\n",
        "        # This layer will combine the spatial and frequency info to create the gate\n",
        "        self.gate_conv = nn.Sequential(\n",
        "            nn.Conv2d(skip_channels + decoder_channels + shearlet_channels,\n",
        "                      skip_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(skip_channels, skip_channels, kernel_size=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, s_i, d_i, f_shear):\n",
        "        # S_i: Skip features from encoder\n",
        "        # D_i: Decoder features from previous (deeper) stage\n",
        "        # F_shear: Global frequency features from LSF\n",
        "\n",
        "        # Resize decoder and shearlet features to match the skip connection's size\n",
        "        target_size = s_i.shape[2:]\n",
        "        d_i_resized = nn.functional.interpolate(d_i, size=target_size, mode='bilinear')\n",
        "        f_shear_resized = nn.functional.interpolate(f_shear, size=target_size, mode='bilinear')\n",
        "\n",
        "        # Concatenate all features\n",
        "        combined_features = torch.cat([s_i, d_i_resized, f_shear_resized], dim=1)\n",
        "\n",
        "        # Generate the attention gate\n",
        "        gate = self.gate_conv(combined_features)\n",
        "\n",
        "        # Apply the gate to the original skip connection\n",
        "        return s_i * gate"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-16T08:12:07.699885Z",
          "iopub.status.idle": "2025-10-16T08:12:07.700241Z",
          "shell.execute_reply.started": "2025-10-16T08:12:07.700089Z",
          "shell.execute_reply": "2025-10-16T08:12:07.700106Z"
        },
        "id": "C-M4ckV-GRUf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# The PEM just needs to produce a single logit per pixel\n",
        "class PerPixelExitModule(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.exit_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels // 2, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels // 2, 1, kernel_size=1) # Output one logit per pixel\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.exit_conv(x)\n",
        "\n",
        "# The supervision head is a standard segmentation classifier\n",
        "class SegmentationHead(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-16T08:12:07.700998Z",
          "iopub.status.idle": "2025-10-16T08:12:07.701302Z",
          "shell.execute_reply.started": "2025-10-16T08:12:07.701151Z",
          "shell.execute_reply": "2025-10-16T08:12:07.701167Z"
        },
        "id": "6NuBRrlQGRUg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_loss_and_metric_curves(train_loss_history, val_loss_history, val_iou_history, val_accuracy_history):\n",
        "    \"\"\"Plots training/validation loss and validation metrics curves.\"\"\"\n",
        "\n",
        "    plt.style.use('dark_background')\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    epochs = range(1, len(train_loss_history) + 1)\n",
        "\n",
        "    # Plotting the training and validation loss\n",
        "    axes[0].plot(epochs, train_loss_history, 'o-', color='cyan', label='Training Loss')\n",
        "    axes[0].plot(epochs, val_loss_history, 'o-', color='magenta', label='Validation Loss')\n",
        "    axes[0].set_title('Training and Validation Loss', fontsize=16, color='white')\n",
        "    axes[0].set_xlabel('Epochs', fontsize=12, color='white')\n",
        "    axes[0].set_ylabel('Loss', fontsize=12, color='white')\n",
        "    axes[0].grid(True, which='both', linestyle='--', linewidth=0.5, color='gray')\n",
        "    axes[0].tick_params(axis='x', colors='white')\n",
        "    axes[0].tick_params(axis='y', colors='white')\n",
        "    axes[0].legend(frameon=True).get_frame().set_facecolor('black')\n",
        "\n",
        "\n",
        "    # Plotting the validation metrics\n",
        "    axes[1].plot(epochs, val_iou_history, 'o-', color='lime', label='Validation IoU (macro)')\n",
        "    axes[1].plot(epochs, val_accuracy_history, 'o-', color='yellow', label='Validation Accuracy (macro)')\n",
        "    axes[1].set_title('Validation Metrics', fontsize=16, color='white')\n",
        "    axes[1].set_xlabel('Epochs', fontsize=12, color='white')\n",
        "    axes[1].set_ylabel('Score', fontsize=12, color='white')\n",
        "    axes[1].grid(True, which='both', linestyle='--', linewidth=0.5, color='gray')\n",
        "    axes[1].tick_params(axis='x', colors='white')\n",
        "    axes[1].tick_params(axis='y', colors='white')\n",
        "    axes[1].legend(frameon=True).get_frame().set_facecolor('black')\n",
        "\n",
        "\n",
        "    # Change spine colors for both subplots\n",
        "    for ax in axes:\n",
        "        for spine in ax.spines.values():\n",
        "            spine.set_edgecolor('gray')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-16T08:12:07.70282Z",
          "iopub.status.idle": "2025-10-16T08:12:07.703077Z",
          "shell.execute_reply.started": "2025-10-16T08:12:07.702927Z",
          "shell.execute_reply": "2025-10-16T08:12:07.702942Z"
        },
        "id": "4gn_ImoDGRUi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import rasterio\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "# --- Constants and Color Map ---\n",
        "CLASS_NAMES = [\"Building\",\n",
        "               \"Agriculture land\",\n",
        "               \"Water\",\n",
        "               \"Tree\",\n",
        "               \"Road\",\n",
        "               \"Developed space\",\n",
        "               \"Rangeland\",\n",
        "               \"Bareland\",\n",
        "               \"Background\",\n",
        "]\n",
        "COLOR_MAP = [\"#DE1F07\",\n",
        "             \"#4BB549\",\n",
        "             \"#0045FF\",\n",
        "             \"#226126\",\n",
        "             \"#FFFFFF\",\n",
        "             \"#949494\",\n",
        "             \"#00FF24\",\n",
        "             \"#800000\",\n",
        "             \"#000000\",\n",
        "            ]\n",
        "\n",
        "def hex_to_rgb(hex_code):\n",
        "    \"\"\"Converts a hex color string to an (R, G, B) tuple.\"\"\"\n",
        "    hex_code = hex_code.lstrip('#')\n",
        "    # This converts the hex pairs (e.g., 'DE', '1F', '07') into integers\n",
        "    return tuple(int(hex_code[i:i+2], 16) for i in (0, 2, 4))\n",
        "\n",
        "color_map_rgb = [hex_to_rgb(hex_val) for hex_val in COLOR_MAP]\n",
        "\n",
        "color_map_np = np.array(color_map_rgb, dtype=np.uint8)\n",
        "\n",
        "# --- 1. UPDATED: Helper Functions ---\n",
        "\n",
        "def denormalize(tensor, mean, std):\n",
        "    \"\"\"Denormalizes an image tensor for visualization.\"\"\"\n",
        "    # Clone to avoid modifying the original tensor\n",
        "    tensor = tensor.clone()\n",
        "    for t, m, s in zip(tensor, mean, std):\n",
        "        t.mul_(s).add_(m)\n",
        "    # Clamp values to [0, 1] range and convert to numpy\n",
        "    return torch.clamp(tensor, 0, 1).permute(1, 2, 0).numpy()\n",
        "\n",
        "def mask_to_rgb(mask, color_map):\n",
        "    \"\"\"\n",
        "    Converts a segmentation mask to a color image using a vectorized approach.\n",
        "    \"\"\"\n",
        "    # Use the mask as indices to directly access colors from the color map\n",
        "    return color_map[mask]\n",
        "\n",
        "def calculate_percentages(mask, class_names):\n",
        "    \"\"\"Calculates the percentage of each class in a mask.\"\"\"\n",
        "    total_pixels = mask.size\n",
        "    unique, counts = np.unique(mask, return_counts=True)\n",
        "    legend_data = {}\n",
        "    for class_id, count in zip(unique, counts):\n",
        "        if class_id < len(class_names):\n",
        "            percentage = (count / total_pixels) * 100\n",
        "            legend_data[class_names[class_id]] = percentage\n",
        "    return legend_data\n",
        "\n",
        "# --- 2. UPDATED: Core Prediction Function ---\n",
        "\n",
        "def predict(model, image_tensor, device):\n",
        "    \"\"\"Performs inference on a single image tensor.\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "        # Model in eval mode with FPN returns the final, upsampled prediction\n",
        "        prediction = model(image_tensor, training=False)\n",
        "\n",
        "        # Get the predicted class index for each pixel\n",
        "        pred_mask = torch.argmax(prediction, dim=1).squeeze(0).cpu().numpy()\n",
        "\n",
        "    return pred_mask\n",
        "\n",
        "# --- 3. UPDATED: Main Visualization Function ---\n",
        "\n",
        "def show_predictions(model, data_loader, device, num_samples=3):\n",
        "    \"\"\"\n",
        "    Runs predictions on samples from the data loader and visualizes the results.\n",
        "    \"\"\"\n",
        "    # Set plot style\n",
        "    plt.style.use('default')\n",
        "\n",
        "    # Get a single batch of data\n",
        "    images, true_masks = next(iter(data_loader))\n",
        "\n",
        "    # Define ImageNet statistics for denormalization\n",
        "    imagenet_mean = [0.485, 0.456, 0.406]\n",
        "    imagenet_std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        image_tensor = images[i]\n",
        "        true_mask_np = true_masks[i].numpy()\n",
        "\n",
        "        # Get model prediction\n",
        "        predicted_mask_np = predict(model, image_tensor, device)\n",
        "\n",
        "        # Convert masks to color images for visualization\n",
        "        true_mask_rgb = mask_to_rgb(true_mask_np, color_map_np)\n",
        "        predicted_mask_rgb = mask_to_rgb(predicted_mask_np, color_map_np)\n",
        "\n",
        "        # Denormalize original image for correct display\n",
        "        original_img_np = denormalize(image_tensor, imagenet_mean, imagenet_std)\n",
        "\n",
        "        # Get legend data\n",
        "        legend_data = calculate_percentages(predicted_mask_np, CLASS_NAMES)\n",
        "\n",
        "        # Plotting\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "\n",
        "        axes[0].imshow(original_img_np)\n",
        "        axes[0].set_title(\"Original Image\")\n",
        "\n",
        "        axes[1].imshow(true_mask_rgb)\n",
        "        axes[1].set_title(\"Ground Truth Mask\")\n",
        "\n",
        "        axes[2].imshow(predicted_mask_rgb)\n",
        "        axes[2].set_title(\"Predicted Mask\")\n",
        "\n",
        "        for ax in axes:\n",
        "            ax.axis('off')\n",
        "\n",
        "        # Create a legend for the predicted mask percentages\n",
        "        legend_patches = [Patch(color=np.array(COLOR_MAP[CLASS_NAMES.index(name)])/255.,\n",
        "                                label=f\"{name}: {perc:.2f}%\")\n",
        "                          for name, perc in sorted(legend_data.items(), key=lambda item: item[1], reverse=True)]\n",
        "\n",
        "        fig.legend(handles=legend_patches, bbox_to_anchor=(1.05, 0.75), loc='upper left', borderaxespad=0.)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-16T08:12:07.704546Z",
          "iopub.status.idle": "2025-10-16T08:12:07.70488Z",
          "shell.execute_reply.started": "2025-10-16T08:12:07.704725Z",
          "shell.execute_reply": "2025-10-16T08:12:07.704742Z"
        },
        "id": "4i1-AefhGRUi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the Dice Loss for multi-class semantic segmentation.\n",
        "    \"\"\"\n",
        "    def __init__(self, smooth=1e-6):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            logits: A tensor of shape (B, C, H, W) from the model.\n",
        "            targets: A tensor of shape (B, H, W) with ground truth indices.\n",
        "        \"\"\"\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        num_classes = logits.shape[1]\n",
        "        targets_one_hot = F.one_hot(targets, num_classes=num_classes).permute(0, 3, 1, 2).float()\n",
        "\n",
        "        probs_flat = probs.contiguous().view(-1)\n",
        "        targets_flat = targets_one_hot.contiguous().view(-1)\n",
        "\n",
        "        intersection = (probs_flat * targets_flat).sum()\n",
        "        total_pixels = probs_flat.sum() + targets_flat.sum()\n",
        "\n",
        "        dice_score = (2. * intersection + self.smooth) / (total_pixels + self.smooth)\n",
        "        return 1 - dice_score"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-16T08:12:07.706181Z",
          "iopub.status.idle": "2025-10-16T08:12:07.706448Z",
          "shell.execute_reply.started": "2025-10-16T08:12:07.706325Z",
          "shell.execute_reply": "2025-10-16T08:12:07.706336Z"
        },
        "id": "7sgpDxUBGRUj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    A robust implementation of Focal Loss, designed to be a drop-in\n",
        "    replacement for nn.CrossEntropyLoss.\n",
        "    \"\"\"\n",
        "    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
        "        \"\"\"\n",
        "        :param gamma: The focusing parameter (gamma > 0). Default is 2.0.\n",
        "        :param alpha: Class weights. A tensor of shape (C,) where C is num_classes.\n",
        "        :param reduction: 'mean', 'sum', or 'none'.\n",
        "        \"\"\"\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.reduction = reduction\n",
        "\n",
        "        # Move alpha to device if provided\n",
        "        if self.alpha is not None:\n",
        "            if not isinstance(self.alpha, torch.Tensor):\n",
        "                self.alpha = torch.tensor(self.alpha)\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        \"\"\"\n",
        "        :param logits: Model predictions (raw logits) of shape [B, C, H, W]\n",
        "        :param targets: Ground truth labels of shape [B, H, W]\n",
        "        \"\"\"\n",
        "\n",
        "        # Use the 'weight' parameter of cross_entropy for alpha\n",
        "        # and 'reduction=none' to get the loss for each pixel\n",
        "        ce_loss = F.cross_entropy(\n",
        "            logits,\n",
        "            targets,\n",
        "            weight=self.alpha.to(logits.device) if self.alpha is not None else None,\n",
        "            reduction='none'\n",
        "        )\n",
        "\n",
        "        # pt = exp(-ce_loss)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "\n",
        "        # Calculate the focal loss\n",
        "        # (1 - pt)^gamma * ce_loss\n",
        "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
        "\n",
        "        # Apply the reduction\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss"
      ],
      "metadata": {
        "id": "MrGvoGyTN2CC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7GzTOncibPQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c00ea19-a193-4427-b8ef-0f2f8d8279c0"
      },
      "source": [
        "!pip install -q torchmetrics"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/983.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.1/983.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "20t-w7OwizOQ",
        "outputId": "5d2748a2-a8d7-4fda-f7ba-0f2fa9fed6da"
      },
      "source": [
        "from torchmetrics import JaccardIndex, Accuracy\n",
        "\n",
        "def get_file_paths(split_txt_path):\n",
        "    \"\"\"\n",
        "    Reads a split file, constructs full paths, and verifies that both the\n",
        "    image and its corresponding label file actually exist before adding them.\n",
        "    \"\"\"\n",
        "    image_paths, label_paths = [], []\n",
        "    with open(split_txt_path, 'r') as f:\n",
        "        filenames = [line.strip() for line in f.readlines()]\n",
        "\n",
        "    split_name = os.path.basename(split_txt_path).split('.')[0]\n",
        "\n",
        "    print(f\"Verifying files for split: {split_name}...\")\n",
        "    for fname in tqdm(filenames, desc=\"Checking file existence\"):\n",
        "        image_path = os.path.join(cfg.IMAGE_BASE_DIR, split_name, fname)\n",
        "        label_path = os.path.join(cfg.LABEL_BASE_DIR, split_name, fname)\n",
        "\n",
        "        if os.path.exists(image_path) and os.path.exists(label_path):\n",
        "            image_paths.append(image_path)\n",
        "            label_paths.append(label_path)\n",
        "\n",
        "    print(f\"Found {len(image_paths)} valid file pairs for the '{split_name}' set.\")\n",
        "    return image_paths, label_paths\n",
        "\n",
        "def main():\n",
        "    # 1. Prepare Data\n",
        "    train_imgs, train_masks = get_file_paths(cfg.TRAIN_TXT)\n",
        "    val_imgs, val_masks = get_file_paths(cfg.VAL_TXT)\n",
        "\n",
        "    train_dataset = LandCoverDataset(train_imgs, train_masks, transform=train_transform)\n",
        "    val_dataset = LandCoverDataset(val_imgs, val_masks, transform=val_transform)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=cfg.BATCH_SIZE, num_workers=cfg.NUM_WORKERS,\n",
        "        pin_memory=cfg.PIN_MEMORY, shuffle=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=cfg.BATCH_SIZE, num_workers=cfg.NUM_WORKERS,\n",
        "        pin_memory=cfg.PIN_MEMORY, shuffle=False\n",
        "    )\n",
        "\n",
        "    # 2. Initialize Model\n",
        "    model = ShearletExitUNet(\n",
        "        in_channels=3,\n",
        "        num_classes=cfg.NUM_CLASSES,\n",
        "        shearlet_features=8,\n",
        "        dropout_prob=0.4, # Slightly increased dropout for more regularization\n",
        "        fpn_out_channels=cfg.FPN_CHANNELS,\n",
        "        backbone_name='resnet50'\n",
        "    ).to(cfg.DEVICE)\n",
        "\n",
        "    # --- 3. Define Loss, Optimizer, and Scheduler ---\n",
        "\n",
        "    # Define weighted loss for early exits. Give the final exit the most weight.\n",
        "    # The number of elements should match the number of prediction heads in your model.\n",
        "    # Adjust exit_weights based on the number of features returned by the new backbone\n",
        "    num_encoder_features = len(model.encoder.output_channels)\n",
        "    exit_weights = [i / num_encoder_features for i in range(1, num_encoder_features + 1)]\n",
        "\n",
        "\n",
        "    # Use class weights if you have class imbalance\n",
        "    # class_weights = torch.tensor([...]).to(cfg.DEVICE)\n",
        "    # loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    loss_fn_ce = FocalLoss(gamma=2.0)\n",
        "    loss_fn_dice = DiceLoss()\n",
        "\n",
        "    loss_alpha = 0.5\n",
        "    loss_beta = 0.5\n",
        "\n",
        "    # Use a differential learning rate for the optimizer\n",
        "    # This uses a smaller LR for the pretrained encoder and a larger one for the new decoder.\n",
        "    encoder_params = model.encoder.parameters()\n",
        "    decoder_params = [p for name, p in model.named_parameters() if not name.startswith(\"encoder.\")]\n",
        "    optimizer = optim.AdamW([\n",
        "        {'params': encoder_params, 'lr': cfg.LEARNING_RATE * 0.1},\n",
        "        {'params': decoder_params, 'lr': cfg.LEARNING_RATE}\n",
        "    ], weight_decay=cfg.WEIGHT_DECAY)\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    scheduler = lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=len(train_loader) * cfg.EPOCHS, # Total number of training steps\n",
        "        eta_min=1e-6 # A very small minimum learning rate\n",
        "    )\n",
        "\n",
        "    # --- Initialize Metrics ---\n",
        "    # Use 'macro' averaging to calculate the mean IoU across all classes\n",
        "    val_iou = JaccardIndex(task='multiclass', num_classes=cfg.NUM_CLASSES, average='macro').to(cfg.DEVICE)\n",
        "    # Use 'macro' average for Accuracy in multiclass\n",
        "    val_accuracy = Accuracy(task='multiclass', num_classes=cfg.NUM_CLASSES, average='macro').to(cfg.DEVICE)\n",
        "\n",
        "    # 4. Run Training Loop...\n",
        "    train_loss_history = []\n",
        "    val_loss_history = []\n",
        "    val_iou_history = []\n",
        "    val_accuracy_history = []\n",
        "    best_val_iou = float('-inf')\n",
        "    epochs_no_improve = 0\n",
        "    early_stop = False\n",
        "\n",
        "\n",
        "    for epoch in range(cfg.EPOCHS):\n",
        "        if early_stop:\n",
        "            print(f\"Early stopping triggered at epoch {epoch}.\")\n",
        "            break\n",
        "\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{cfg.EPOCHS}\")\n",
        "\n",
        "        for data, targets in loop:\n",
        "            data = data.to(cfg.DEVICE)\n",
        "            targets_on_device = targets.to(cfg.DEVICE) # Move targets to device\n",
        "\n",
        "            all_predictions = model(data, training=True)\n",
        "\n",
        "            # --- Weighted Loss Calculation ---\n",
        "            total_loss = 0\n",
        "            for i, pred in enumerate(all_predictions):\n",
        "                # Resize the single target tensor once for all predictions of the same size\n",
        "                target_resized = F.interpolate(\n",
        "                    targets_on_device.unsqueeze(1).float(),\n",
        "                    size=pred.shape[2:],\n",
        "                    mode='nearest'\n",
        "                ).squeeze(1).long()\n",
        "\n",
        "                ce_loss = loss_fn_ce(pred, target_resized)\n",
        "                dice_loss = loss_fn_dice(pred, target_resized)\n",
        "\n",
        "                combined_loss = (loss_alpha * ce_loss) + (loss_beta * dice_loss)\n",
        "\n",
        "                total_loss += exit_weights[i] * combined_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            scaler.scale(total_loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step() # Step the scheduler after each batch\n",
        "\n",
        "            train_loss += total_loss.item()\n",
        "            loop.set_postfix(loss=total_loss.item())\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        train_loss_history.append(avg_train_loss)\n",
        "\n",
        "        # --- Validation Loop ---\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_iou.reset() # Reset metrics at the beginning of each validation epoch\n",
        "        val_accuracy.reset()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, targets in val_loader:\n",
        "                data = data.to(cfg.DEVICE)\n",
        "                targets_on_device = targets.to(cfg.DEVICE)\n",
        "\n",
        "                # In evaluation mode, model returns the final prediction only\n",
        "                final_prediction_logits = model(data, training=False)\n",
        "\n",
        "                # For metric calculation, we need the predicted class index\n",
        "                final_prediction_mask = torch.argmax(final_prediction_logits, dim=1)\n",
        "\n",
        "                # Calculate loss for the final prediction only during validation logging\n",
        "                val_loss += loss_fn_ce(final_prediction_logits, targets_on_device).item()\n",
        "                val_loss += loss_fn_dice(final_prediction_logits, targets_on_device).item()\n",
        "\n",
        "                # Update metrics\n",
        "                val_iou.update(final_prediction_mask, targets_on_device)\n",
        "                val_accuracy.update(final_prediction_mask, targets_on_device)\n",
        "\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        avg_val_iou = val_iou.compute()\n",
        "        avg_val_accuracy = val_accuracy.compute()\n",
        "\n",
        "        val_loss_history.append(avg_val_loss)\n",
        "        val_iou_history.append(avg_val_iou.item()) # Append scalar value\n",
        "        val_accuracy_history.append(avg_val_accuracy.item()) # Append scalar value\n",
        "\n",
        "\n",
        "        print(f\"Epoch {epoch+1} Summary -> Avg Train Loss: {avg_train_loss:.4f} | Avg Val Loss: {avg_val_loss:.4f} | Avg Val IoU: {avg_val_iou:.4f} | Avg Val Acc: {avg_val_accuracy:.4f}\")\n",
        "\n",
        "        # --- Early Stopping Check ---\n",
        "        if avg_val_iou > best_val_iou + cfg.MIN_DELTA:\n",
        "            best_val_iou = avg_val_iou\n",
        "            epochs_no_improve = 0\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            print(f\"Mean IoU improved. Saving model. Best IoU: {best_val_iou:.4f}\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            print(f\"Mean IoU did not improve for {epochs_no_improve} epochs.\")\n",
        "            if epochs_no_improve >= cfg.EARLY_STOPPING_PATIENCE:\n",
        "                early_stop = True\n",
        "\n",
        "    print(\"Training finished.\")\n",
        "    plot_loss_and_metric_curves(train_loss_history, val_loss_history, val_iou_history, val_accuracy_history)\n",
        "\n",
        "    # Optionally plot metrics as well\n",
        "    # plot_metric_curves(val_iou_history, val_accuracy_history)\n",
        "\n",
        "\n",
        "    print(\"Loading best model for final predictions...\")\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    show_predictions(model, val_loader, cfg.DEVICE)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verifying files for split: train...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Checking file existence: 100%|██████████| 3000/3000 [00:00<00:00, 82867.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2303 valid file pairs for the 'train' set.\n",
            "Verifying files for split: val...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Checking file existence: 100%|██████████| 500/500 [00:00<00:00, 64971.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 384 valid file pairs for the 'val' set.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/tmp/ipython-input-2713401457.py:80: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n",
            "Epoch 1/40:   0%|          | 0/576 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Epoch 1/40:   0%|          | 2/576 [01:03<5:03:02, 31.68s/it, loss=3.85]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2713401457.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2713401457.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}